{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-07T23:44:32.024053Z",
     "iopub.status.busy": "2023-06-07T23:44:32.023344Z",
     "iopub.status.idle": "2023-06-07T23:44:34.019077Z",
     "shell.execute_reply": "2023-06-07T23:44:34.017737Z",
     "shell.execute_reply.started": "2023-06-07T23:44:32.024017Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Función que te permite obtener la fila i-ésima de la columna 'hist'\n",
    "#Se introduce el dataframe y el número de fila\n",
    "def str_array(df, i):\n",
    "    return [np.array(eval(df.values[i])).astype(int)]\n",
    "\n",
    "\n",
    "# Función para generar la matriz con las componentes principales tras el entrenamiento\n",
    "#Se introduce el diccionario con las propiedades separadas, el diccionario de matrices V de componentes principales, la propiedad que se entrena y el número de componentes\n",
    "# El quinto parámetro permite obtener información sobre la varianza y el número de componentes principales utilizadas si se hace igual a 1\n",
    "def training(dic_prop_era, dic_V, prop, era, PC, exp):\n",
    "    matriz = np.zeros((1,np.shape(dic_V['V{}A'.format(prop)])[1]))  #Se crea una fila de ceros para concatenar sobre ella, luego se elimina\n",
    "    for i in era:\n",
    "        ind_buenos = np.array(dic_prop_era['{}{}'.format(prop,i)][dic_prop_era['{}{}'.format(prop,i)]['labels']==True]['new_lumi'].astype(int))\n",
    "        datos = dic_V['V{}{}'.format(prop, i)][ind_buenos]\n",
    "        matriz = np.concatenate((matriz,datos), axis=0)\n",
    "    matriz = np.delete(matriz,0,axis=0)\n",
    "    pca = PCA(n_components = PC)\n",
    "    pca.fit(matriz)\n",
    "    if exp==1:\n",
    "        print('Varianza explicada por componente:', pca.explained_variance_ratio_*100)\n",
    "        print('Varianza explicada total:', np.sum(pca.explained_variance_ratio_*100))\n",
    "        print('Número de componentes principales:', pca.n_components_)\n",
    "    return pca.components_\n",
    "\n",
    "\n",
    "#Función que permite evaluar el modelo. Se introduce la matriz V de los datos a evaluar y las componentes tras el entrenamiento\n",
    "def evaluation(datos, componentes):\n",
    "    recon = np.matmul(np.matmul(datos, componentes.T), componentes) #Cada fila es un histograma reconstruido normalizado\n",
    "    norma = np.max([np.max(datos, axis = 1), np.max(recon, axis = 1)], axis = 0) #Para calcular el MSE se normalizan los histogramas\n",
    "    mse = (((datos-recon)/norma[:, None])**2).sum(axis=1)/np.shape(datos)[1]\n",
    "    return (recon, mse)\n",
    "\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "#Función que determina las regiones anómalas\n",
    "#Se introducen los datos suavizados y la anchura mínima que deben tener los picos para ser señalados como datos malos\n",
    "def anomalias(datos_suavizados, anchura):\n",
    "    maximos,_ = find_peaks(datos_suavizados, width=150, distance=50) #Se determinan los máximos asociados a fills\n",
    "    picos, picosinfo = find_peaks(datos_suavizados, width=anchura, distance=50, prominence=np.min(datos_suavizados)/2) #Picos anómalos en los datos suavizados\n",
    "    semianchuras = np.asarray(np.floor(picosinfo['widths']/2), dtype='int')\n",
    "    descartes = np.array([picos, semianchuras])  #Matriz. Primera fila son los picos, la segunda sus semianchuras como números enteros\n",
    "    descartes = descartes[:, ~np.isin(descartes[0],maximos)]  #Quitamos los picos que sean máximos debidos a fills\n",
    "    descartes = descartes[:, descartes[0]<(len(datos_suavizados)-150)]\n",
    "    descartes = descartes[:, descartes[0]>200]\n",
    "    extra = [0]\n",
    "    for n in range(200,len(datos_suavizados)-200):\n",
    "        if datos_suavizados[n]>2*np.median(datos_suavizados[(n-10):n]) and extra[len(extra)-1]<(n-100):\n",
    "            extra = np.append(extra,n)\n",
    "            k = 1\n",
    "            while datos_suavizados[n+k] < 1.5*datos_suavizados[n+k+50]:\n",
    "                k = k+1\n",
    "            descartes = np.c_[descartes, [n+np.floor(k/2), np.floor(k/2)]]\n",
    "    descartes = descartes.astype(int)\n",
    "    return descartes, maximos\n",
    "\n",
    "#Función análoga a la interior pero adaptada a las particularidades de chi2OverDf\n",
    "def anomaliaschi(datos_suavizados, anchura):\n",
    "    picos, picosinfo = find_peaks(datos_suavizados, width=anchura, distance=10, prominence=np.min(datos_suavizados))\n",
    "    semianchuras = np.asarray(np.floor(picosinfo['widths']/2), dtype='int')\n",
    "    descartes = np.array([picos, semianchuras])  #Matriz. Primera fila son los picos, la segunda sus semianchuras como números enteros\n",
    "    descartes = descartes[:, descartes[0]<(len(datos_suavizados)-150)]\n",
    "    descartes = descartes[:, descartes[0]>200]\n",
    "    return descartes\n",
    "\n",
    "\n",
    "#Función para determinar el parámetro de corte y los datos descartados a partir de él\n",
    "#Se introduce el vector de mse, los datos suavizados y los máximos debidos a fills calculados con la función anomalias\n",
    "def corte(datos, datos_suavizados, maximos):\n",
    "    id = np.max(np.where(datos_suavizados[0]==np.max(datos_suavizados[0][maximos]))) #Se determina el índice en el que se alcanza el máximo de la mediana móvil\n",
    "    vmax = 0\n",
    "    #Se escanea la zona alrededor de dicho máximo\n",
    "    for n in range(id-200,id+200):\n",
    "        if datos[n]>vmax and datos[n]<2*np.median(datos[(n-100):n]):\n",
    "            vmax = datos[n]\n",
    "            id2 = n\n",
    "    falsos_corte = np.where(datos>vmax)[0] #Índices de los valores descartados por superar el parámetro de corte\n",
    "    return vmax, falsos_corte\n",
    "\n",
    "#Análoga de la función anterior adaptada a las particularidades de chi2OverDf\n",
    "#Se introduce únicamente el mse\n",
    "def cortechi(datos):\n",
    "    id = np.where(datos>np.percentile(datos,99))[0]\n",
    "    for n in id:\n",
    "        if(datos[n]<10*np.median(datos[(n-min(n,100)):(n+min(len(datos)-n, 100))])):\n",
    "            id = id[id!=n]\n",
    "    if len(id)>0:\n",
    "        vmax = min(datos[id])\n",
    "    else:\n",
    "        vmax = 10**10\n",
    "    falsos_corte = np.where(datos>vmax)[0]\n",
    "    return vmax, falsos_corte\n",
    "\n",
    "#Permite obtener los datos etiquetados como malos por el modelo completo\n",
    "#Se introduce el diccionario de conjuntos, el diccionario de matrices V, una lista con las eras de entrenamiento, una con las de evaluación y una con el número de PC\n",
    "def datos_malos(dic_prop_era,dic_V,entrenamiento,evaluacion,n):\n",
    "    malos = np.array([0])\n",
    "    k = 0\n",
    "    for i in ['eta','chi2OverDf','phi','pt']:\n",
    "        matriz = np.zeros((1,np.shape(dic_V['V{}A'.format(i)])[1]))\n",
    "        for t in evaluacion:\n",
    "            datos_eval = dic_V['V{}{}'.format(i, t)]\n",
    "            matriz = np.concatenate((matriz,datos_eval), axis=0)\n",
    "        matriz = np.delete(matriz,0,axis=0)\n",
    "        if type(n)==int:\n",
    "            recon, mse = evaluation(matriz, training(dic_prop_era, dic_V, i, entrenamiento, n, 0))\n",
    "        else:\n",
    "            recon, mse = evaluation(matriz, training(dic_prop_era, dic_V, i, entrenamiento, n[k], 0))\n",
    "            k = k+1\n",
    "        mse_df = pd.DataFrame(mse)\n",
    "        smoothed_mse_median = mse_df.rolling(window = 100, center=True).apply(np.median)\n",
    "        if i=='chi2OverDf':\n",
    "            descartes = anomaliaschi(smoothed_mse_median[0], 10**10)\n",
    "            vmax, falsos_corte = cortechi(mse)\n",
    "        else:\n",
    "            descartes, maximos = anomalias(smoothed_mse_median[0], 30)\n",
    "            vmax, falsos_corte = corte(mse, smoothed_mse_median, maximos)\n",
    "        falsos = falsos_corte\n",
    "        for j in range(0,len(descartes[0])):\n",
    "            ei = descartes[0,j]-descartes[1,j]\n",
    "            ed = descartes[0,j]+descartes[1,j]\n",
    "            e = 2*descartes[1,j]+1\n",
    "            falsos = np.concatenate((falsos,np.linspace(ei,ed,e)))\n",
    "        malos = np.concatenate((malos,falsos))\n",
    "    malos = np.unique(malos[malos!=0]).astype(int)\n",
    "    return malos, mse\n",
    "\n",
    "#Análoga de la función anterior pero usa una única propiedad como criterio de descarte\n",
    "#Se añade el parámetro que indica la propiedad usada como un string\n",
    "def datos_malos_PC(dic_prop_era,dic_V,entrenamiento,evaluacion,n,prop):\n",
    "    malos = np.array([0])\n",
    "    k = 0\n",
    "    for i in ['{}'.format(prop)]:\n",
    "        matriz = np.zeros((1,np.shape(dic_V['V{}A'.format(i)])[1]))\n",
    "        for t in evaluacion:\n",
    "            datos_eval = dic_V['V{}{}'.format(i, t)]\n",
    "            matriz = np.concatenate((matriz,datos_eval), axis=0)\n",
    "        matriz = np.delete(matriz,0,axis=0)\n",
    "        if type(n)==int:\n",
    "            recon, mse = evaluation(matriz, training(dic_prop_era, dic_V, i, entrenamiento, n, 0))\n",
    "        else:\n",
    "            recon, mse = evaluation(matriz, training(dic_prop_era, dic_V, i, entrenamiento, n[k], 0))\n",
    "            k = k+1\n",
    "        mse_df = pd.DataFrame(mse)\n",
    "        smoothed_mse_median = mse_df.rolling(window = 100, center=True).apply(np.median)\n",
    "        if i=='chi2OverDf':\n",
    "            descartes = anomaliaschi(smoothed_mse_median[0], 10**10)\n",
    "            vmax, falsos_corte = cortechi(mse)\n",
    "        else:\n",
    "            descartes, maximos = anomalias(smoothed_mse_median[0], 30)\n",
    "            vmax, falsos_corte = corte(mse, smoothed_mse_median, maximos)\n",
    "        falsos = falsos_corte\n",
    "        for j in range(0,len(descartes[0])):\n",
    "            ei = descartes[0,j]-descartes[1,j]\n",
    "            ed = descartes[0,j]+descartes[1,j]\n",
    "            e = 2*descartes[1,j]+1\n",
    "            falsos = np.concatenate((falsos,np.linspace(ei,ed,e)))\n",
    "        malos = np.concatenate((malos,falsos))\n",
    "    malos = np.unique(malos[malos!=0]).astype(int)\n",
    "    return malos, mse\n",
    "\n",
    "#Obtiene la matriz de confusión y las medidas resumen\n",
    "#Se introduce el diccionario de conjuntos, la lista de eras de evaluación y los datos malos obtenidos de la función datos_malos\n",
    "#El último parámetro muestra por pantalla los parámetros si es igual a 1\n",
    "def resultados(dic_prop_era,evaluacion,malos,exp):\n",
    "    df = dic_prop_era['eta{}'.format(evaluacion[0])]\n",
    "    etiquetas_expertos = np.array(df['labels'])\n",
    "    etiquetas_modelo = ~np.isin(np.linspace(0,len(etiquetas_expertos),len(etiquetas_expertos)).astype(int),malos)\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(0,len(etiquetas_expertos)):\n",
    "        if etiquetas_expertos[i]==True and etiquetas_modelo[i]==True:\n",
    "            TP = TP+1\n",
    "        elif etiquetas_expertos[i]==False and etiquetas_modelo[i]==False:\n",
    "            TN = TN+1\n",
    "        elif etiquetas_expertos[i]==True and etiquetas_modelo[i]==False:\n",
    "            FN = FN+1\n",
    "        else:\n",
    "            FP = FP+1\n",
    "    PPV = TP/(TP+FP)\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "    F12 = (1+0.25)*PPV*TPR/(0.25*PPV+TPR)\n",
    "    GM = np.sqrt(TPR*TNR)\n",
    "    if exp==1:\n",
    "        print('TP:',TP,'  FN:',FN)\n",
    "        print('FP:',FP,'     TN:',TN)\n",
    "        print('PPV:',PPV)\n",
    "        print('TPR:',TPR)\n",
    "        print('TNR:',TNR)\n",
    "        print('F12:',F12)\n",
    "        print('GM:',GM)\n",
    "    return F12, GM\n",
    "\n",
    "#Análogo de la función anterior para obtener únicamente la matriz de confusión\n",
    "def resultados_prop(dic_prop_era,evaluacion,malos,exp):\n",
    "    df = dic_prop_era['eta{}'.format(evaluacion[0])]\n",
    "    etiquetas_expertos = np.array(df['labels'])\n",
    "    etiquetas_modelo = ~np.isin(np.linspace(0,len(etiquetas_expertos),len(etiquetas_expertos)).astype(int),malos)\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(0,len(etiquetas_expertos)):\n",
    "        if etiquetas_expertos[i]==True and etiquetas_modelo[i]==True:\n",
    "            TP = TP+1\n",
    "        elif etiquetas_expertos[i]==False and etiquetas_modelo[i]==False:\n",
    "            TN = TN+1\n",
    "        elif etiquetas_expertos[i]==True and etiquetas_modelo[i]==False:\n",
    "            FN = FN+1\n",
    "        else:\n",
    "            FP = FP+1\n",
    "    PPV = TP/(TP+FP)\n",
    "    TPR = TP/(TP+FN)\n",
    "    TNR = TN/(TN+FP)\n",
    "    F12 = (1+0.25)*PPV*TPR/(0.25*PPV+TPR)\n",
    "    GM = np.sqrt(TPR*TNR)\n",
    "    if exp==1:\n",
    "        print('TP:',TP,'  FN:',FN)\n",
    "        print('FP:',FP,'     TN:',TN)\n",
    "        print('PPV:',PPV)\n",
    "        print('TPR:',TPR)\n",
    "        print('TNR:',TNR)\n",
    "        print('F12:',F12)\n",
    "        print('GM:',GM)\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "#Análoga de datos_malos pero usando la media en lugar de la mediana\n",
    "def datos_malos_mean(dic_prop_era,dic_V,entrenamiento,evaluacion,n):\n",
    "    malos = np.array([0])\n",
    "    k = 0\n",
    "    for i in ['eta','chi2OverDf','phi','pt']:\n",
    "        matriz = np.zeros((1,np.shape(dic_V['V{}A'.format(i)])[1]))\n",
    "        for t in evaluacion:\n",
    "            datos_eval = dic_V['V{}{}'.format(i, t)]\n",
    "            matriz = np.concatenate((matriz,datos_eval), axis=0)\n",
    "        matriz = np.delete(matriz,0,axis=0)\n",
    "        if type(n)==int:\n",
    "            recon, mse = evaluation(matriz, training(dic_prop_era, dic_V, i, entrenamiento, n, 0))\n",
    "        else:\n",
    "            recon, mse = evaluation(matriz, training(dic_prop_era, dic_V, i, entrenamiento, n[k], 0))\n",
    "            k = k+1\n",
    "        mse_df = pd.DataFrame(mse)\n",
    "        smoothed_mse_mean = mse_df.rolling(window = 100, center=True).mean()\n",
    "        if i=='chi2OverDf':\n",
    "            descartes = anomaliaschi(smoothed_mse_mean[0], 10**10)\n",
    "            vmax, falsos_corte = cortechi(mse)\n",
    "        else:\n",
    "            descartes, maximos = anomalias(smoothed_mse_mean[0], 30)\n",
    "            vmax, falsos_corte = corte(mse, smoothed_mse_mean, maximos)\n",
    "        falsos = falsos_corte\n",
    "        for j in range(0,len(descartes[0])):\n",
    "            ei = descartes[0,j]-descartes[1,j]\n",
    "            ed = descartes[0,j]+descartes[1,j]\n",
    "            e = 2*descartes[1,j]+1\n",
    "            falsos = np.concatenate((falsos,np.linspace(ei,ed,e)))\n",
    "        malos = np.concatenate((malos,falsos))\n",
    "    malos = np.unique(malos[malos!=0]).astype(int)\n",
    "    return malos, mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
